{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e2d93ea-54ef-4f28-a229-308ab0f69815",
   "metadata": {},
   "source": [
    "## –¶–µ–ª—å —Ä–∞–±–æ—Ç—ã:\n",
    "–û–±—É—á–∏—Ç—å –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –Ω–∞ –∫–æ—Ä–ø—É—Å–µ —Ç–µ–∫—Å—Ç–æ–≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ —á–µ–ª–æ–≤–µ–∫–∞, –Ω–∞—É—á–∏—Ç—å—Å—è –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç –≤ —Å—Ç–∏–ª–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ —á–µ–ª–æ–≤–µ–∫–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcf1da8-2a14-4f95-8938-b74db92b7d1e",
   "metadata": {},
   "source": [
    "## –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Ç—Ä–µ–±—É–µ–º—ã—Ö –ø–∞–∫–µ—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26333eba-bdd7-4fc2-82a1-99f9001a97c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Python 3.11\\n%pip install ipywidgets python-dotenv ollama tensorboard\\n# –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–ª–µ—Å–æ –¥–ª—è cuda 12.6 (–≤–µ—Ä—Å–∏—è cuda, GPU –∏ –¥—Ä–∞–π–≤–µ—Ä–∞ GPU –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å–æ–≤–º–µ—Å—Ç–∏–º—ã –º–µ–∂–¥—É —Å–æ–±–æ–π)\\n%pip install --upgrade pip && pip install xformers --index-url https://download.pytorch.org/whl/cu126\\n# –≤–º–µ—Å—Ç–µ —Å xformers —É—Å—Ç–∞–Ω–æ–≤–∏–ª—Å—è torch, —á—Ç–æ–±—ã —É–∑–Ω–∞—Ç—å –µ–≥–æ –≤–µ—Ä—Å–∏—é –∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –∫–æ–º–∞–Ω–¥—É –¥–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏ unsloth –º–æ–∂–Ω–æ –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –∫–æ–º–Ω–∞–¥–æ–π:\\n#!wget -qO- https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py | python -\\n# –≤—ã–ø–æ–ª–Ω–∏–≤ –ø—Ä–µ–¥—ã–¥—É—â—É—é –∫–æ–º–∞–Ω–¥—É, –º—ã –ø–æ–ª—É—á–∏–ª–∏ –∫–æ–º–∞–Ω–¥—É –¥–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏ unsloth:\\n%pip install \"unsloth[cu126-ampere-torch260] @ git+https://github.com/unslothai/unsloth.git\"\\n# flash-attn –∫–æ–º–ø–∏–ª–∏—Ä—É–µ—Ç—Å—è –∫—Ä–∞–π–Ω–µ –¥–æ–ª–≥–æ, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å –∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ —á–µ—Ä–µ–∑ –∫–æ–Ω—Å–æ–ª—å\\n%pip install --no-build-isolation flash-attn\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Python 3.11\n",
    "%pip install ipywidgets python-dotenv ollama tensorboard\n",
    "# –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–ª–µ—Å–æ –¥–ª—è cuda 12.6 (–≤–µ—Ä—Å–∏—è cuda, GPU –∏ –¥—Ä–∞–π–≤–µ—Ä–∞ GPU –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å–æ–≤–º–µ—Å—Ç–∏–º—ã –º–µ–∂–¥—É —Å–æ–±–æ–π)\n",
    "%pip install --upgrade pip && pip install xformers --index-url https://download.pytorch.org/whl/cu126\n",
    "# –≤–º–µ—Å—Ç–µ —Å xformers —É—Å—Ç–∞–Ω–æ–≤–∏–ª—Å—è torch, —á—Ç–æ–±—ã —É–∑–Ω–∞—Ç—å –µ–≥–æ –≤–µ—Ä—Å–∏—é –∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –∫–æ–º–∞–Ω–¥—É –¥–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏ unsloth –º–æ–∂–Ω–æ –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –∫–æ–º–Ω–∞–¥–æ–π:\n",
    "#!wget -qO- https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py | python -\n",
    "# –≤—ã–ø–æ–ª–Ω–∏–≤ –ø—Ä–µ–¥—ã–¥—É—â—É—é –∫–æ–º–∞–Ω–¥—É, –º—ã –ø–æ–ª—É—á–∏–ª–∏ –∫–æ–º–∞–Ω–¥—É –¥–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏ unsloth:\n",
    "%pip install \"unsloth[cu126-ampere-torch260] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "# flash-attn –∫–æ–º–ø–∏–ª–∏—Ä—É–µ—Ç—Å—è –∫—Ä–∞–π–Ω–µ –¥–æ–ª–≥–æ, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å –∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ —á–µ—Ä–µ–∑ –∫–æ–Ω—Å–æ–ª—å\n",
    "%pip install --no-build-isolation flash-attn\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a76fb8-80c2-4646-88dd-4503b5f49eee",
   "metadata": {},
   "source": [
    "## –ì–µ–Ω–µ—Ä–∞—Ü–∏—è Prompt –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å –ø–æ–º–æ—â—å—é Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ebc86bf-8480-422e-9fd6-0e378194e138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∏ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ollama\n",
    "import os\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "# –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º\n",
    "input_file = \"datasets/Lebedev_messages_5436.csv\"\n",
    "output_file = f\"{input_file[:-4]}_with_prompt.csv\"\n",
    "ollama_used = False\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–º–ø—Ç–∞\n",
    "def generate_prompt(message):\n",
    "    global ollama_used\n",
    "    ollama_used = True\n",
    "    prompt_style = \"\"\"–í—ã –≤–µ–¥–µ—Ç–µ –∫–æ–Ω—Ç–µ–Ω—Ç-–ø–ª–∞–Ω –±–ª–æ–≥–µ—Ä–∞, –ø–∏—Å–∞—Ç–µ–ª—è, —É –∫–æ—Ç–æ—Ä–æ–≥–æ —Å–≤–æ–π —Å—Ç–∏–ª—å –ø–∏—Å—å–º–∞. \n",
    "    –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å–æ—Å—Ç–∞–≤—å—Ç–µ –∫—Ä–∞—Ç–∫–æ–µ –∑–∞–¥–∞–Ω–∏–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –≤ –ø—Ä–æ—Å—Ç–æ–π —Ñ–æ—Ä–º–µ –¥–ª—è –Ω–∞–ø–∏—Å–∞–Ω–∏—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–µ–∫—Å—Ç–∞. –ü—Ä–µ–¥—Å—Ç–∞–≤–∏–º, —á—Ç–æ —ç—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –µ—â–µ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ –±–ª–æ–≥–≥–µ—Ä –¥–æ–ª–∂–µ–Ω –µ–≥–æ –Ω–∞–ø–∏—Å–∞—Ç—å.\n",
    "    –ó–∞–¥–∞–Ω–∏–µ –Ω–µ –¥–æ–ª–∂–Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–ª–∏ –ø—Ä–∏–º–µ—Ä—ã. \n",
    "    \n",
    "    ### –¢–µ–∫—Å—Ç –¥–ª—è –∑–∞–¥–∞–Ω–∏—è:\n",
    "    {}\"\"\"\n",
    "    response = ollama.generate(model=\"deepseek-r1:8b\", prompt=prompt_style.format(message))[\"response\"]\n",
    "    # —É–±–∏—Ä–∞–µ–º –ª–∏—à–Ω—é—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏\n",
    "    response = response.split(\"</think>\")[-1].strip()\n",
    "    return response\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –µ—Å–ª–∏ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç\n",
    "processed_df = pd.read_csv(output_file) if os.path.exists(output_file) else pd.DataFrame()\n",
    "processed_count = len(processed_df)\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏\n",
    "df = pd.read_csv(input_file)\n",
    "remaining_df = df[processed_count:]\n",
    "\n",
    "# –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–∞–º–∏\n",
    "chunk_size = 50\n",
    "for start in range(0, len(remaining_df), chunk_size):\n",
    "    chunk = remaining_df[start:start + chunk_size].copy()\n",
    "    chunk[\"Prompt\"] = chunk[\"Message\"].apply(generate_prompt)\n",
    "\n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–¥–æ–ø–∏—Å—ã–≤–∞–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª)\n",
    "    chunk.to_csv(output_file, mode=\"a\", header=not os.path.exists(output_file), index=False)\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f\"[{current_time}] –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–æ–∫: {processed_count + start + len(chunk)}\")\n",
    "\n",
    "print(\"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∏ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã.\")\n",
    "# –≤—ã–≥—Ä—É–∂–∞–µ–º Ollama\n",
    "if ollama_used:\n",
    "    subprocess.Popen([\"ollama\", \"stop\", \"deepseek-r1:8b\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16a3642f-7bc8-4464-9a16-dd5d4e1611d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "import time\n",
    "if not torch.cuda.is_available():\n",
    "    raise NotImplementedError(\"NVIDIA GPU not found!\") # unsloth —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Å GPU. –î–∞–ª—å–Ω–µ–π—à–µ–µ –≤—ã–ø–æ–ª–Ω–µ–µ –∫–æ–¥–∞ –Ω–µ –∏–º–µ–µ—Ç —Å–º—ã—Å–ª–∞.\n",
    "from unsloth import FastLanguageModel\n",
    "max_seq_length = 2048 \n",
    "dtype = torch.bfloat16 \n",
    "load_in_4bit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03d16e26-b80f-4ae9-bd4f-70fe9c80e8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –≤–∏–¥–µ–æ–ø–∞–º—è—Ç–∏ (VRAM)\n",
    "def clear_vram():\n",
    "    print(\"–û—á–∏—Å—Ç–∫–∞ VRAM...\")\n",
    "    try:\n",
    "        # –°–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞\n",
    "        gc.collect()\n",
    "        # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ CUDA\n",
    "        torch.cuda.empty_cache()\n",
    "        # –°–±–æ—Ä–∫–∞ –º–µ–∂–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–Ω–æ–π –ø–∞–º—è—Ç–∏ CUDA\n",
    "        torch.cuda.ipc_collect()\n",
    "        print(\"VRAM —É—Å–ø–µ—à–Ω–æ –æ—á–∏—â–µ–Ω–∞.\")\n",
    "    except Exception as e:\n",
    "        print(f\"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—á–∏—Å—Ç–∏—Ç—å VRAM: {e}\")\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã–≤–æ–¥–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ VRAM\n",
    "def display_vram_stats(stage=\"before\", start_gpu_memory=0):\n",
    "    gpu_stats = torch.cuda.get_device_properties(0)\n",
    "    current_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "    max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "    if stage == \"before\":\n",
    "        print(f\"GPU = {gpu_stats.name}.\\n–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {current_gpu_memory} GB –∏–∑ {max_memory} GB vRAM.\")\n",
    "        return current_gpu_memory\n",
    "    elif stage == \"after\":\n",
    "        used_memory_for_lora = round(current_gpu_memory - start_gpu_memory, 3)\n",
    "        used_percentage = round(current_gpu_memory / max_memory * 100, 3)\n",
    "        lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "        print(f\"–û–±—É—á–µ–Ω–∏–µ –¥–ª–∏–ª–æ—Å—å {trainer_stats.metrics['train_runtime']} —Å–µ–∫—É–Ω–¥ ({round(trainer_stats.metrics['train_runtime']/60, 2)} –º–∏–Ω—É—Ç).\")\n",
    "        print(f\"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–æ—Å—å vRAM = {current_gpu_memory} –ì–ë.\")\n",
    "        print(f\"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–æ—Å—å vRAM –¥–ª—è –æ–±—É—á–µ–Ω–∏—è = {used_memory_for_lora} –ì–ë.\")\n",
    "        print(f\"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–æ—Å—å vRAM –≤ % –æ—Ç –æ–±—â–µ–π –ø–∞–º—è—Ç–∏ = {used_percentage} %.\")\n",
    "        print(f\"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–æ—Å—å vRAM –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤ % –æ—Ç –æ–±—â–µ–π –ø–∞–º—è—Ç–∏ = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a78d51e-e7c1-40dc-b0d5-de4632c59b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# –ü–æ–ª—É—á–∞–µ–º —Ç–æ–∫–µ–Ω Hugging Face –∏ –∞–≤—Ç–æ—Ä–∏–∑—É–µ–º—Å—è\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "hf_token = os.environ.get('HF_TOKEN')\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed9654a-2aee-482b-b5e7-31cfd82eaf1e",
   "metadata": {},
   "source": [
    "# DeepSeek R1\n",
    "## –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å DeepSeek R1 –¥–æ –æ–±—É—á–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af8f5119-002b-431c-a5fc-ed5a8acb0388",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\envs\\unsloth\\Lib\\site-packages\\unsloth_zoo\\gradient_checkpointing.py:330: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  GPU_BUFFERS = tuple([torch.empty(2*256*2048, dtype = dtype, device = f\"cuda:{i}\") for i in range(n_gpus)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.50.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4070 Ti SUPER. Num GPUs = 1. Max memory: 15.992 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    token = hf_token, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7585baf7-9e02-4e45-a52c-090bb5bb2dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "Write a response that appropriately completes the request. \n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "\n",
    "### Instruction:\n",
    "–í—ã —è–≤–ª—è–µ—Ç–µ—Å—å –ø–∏—Å–∞—Ç–µ–ª–µ–º –∫–Ω–∏–≥ –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –±–ª–æ–≥–≥–µ—Ä–æ–º –∂–∏–≤–æ–≥–æ –∂—É—Ä–Ω–∞–ª–∞, —Å–æ —Å–≤–æ–∏–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–º —Å—Ç–∏–ª–µ–º –Ω–∞–ø–∏—Å–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤. \n",
    "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –Ω–∞–ø–∏—à–∏—Ç–µ —Ç–µ–∫—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –ø–æ —Å–ª–µ–¥—É—é—â–µ–º—É –∑–∞–¥–∞–Ω–∏—é.\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<think>{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12b72d3b-3af9-40aa-b733-05e767d6eb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "–≠–ª–µ–∫—Ç—Ä–æ—á–∏—Å—Ç–∏–ª–∫–∏ –¥–ª—è –æ–±—É–≤–∏ ‚Äî —ç—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –∏ —É–¥–æ–±–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è —Ç–µ—Ö, –∫—Ç–æ —Ü–µ–Ω–∏—ÇÂç´Áîü –∏ –ø–æ—Ä—è–¥–æ–∫. –û–¥–Ω–∞–∫–æ, –∫–∞–∫ –∏‰ªª‰Ωï –¥—Ä—É–≥–æ–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ, –æ–Ω–∏ –∏–º–µ—é—Ç —Å–≤–æ–∏ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏. –í–æ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã—Ö –∞—Å–ø–µ–∫—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ —ç–ª–µ–∫—Ç—Ä–æ—á–∏—Å—Ç–∏–ª–æ–∫:\n",
      "\n",
      "### 1. –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å\n",
      "–≠–ª–µ–∫—Ç—Ä–æ—á–∏—Å—Ç–∏–ª–∫–∏ –¥–ª—è –æ–±—É–≤–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –ø—ã–ª–∏ –∏ –∑–∞–≥—Ä—è–∑–Ω–µ–Ω–∏—è. –ß–∞—Å—Ç—å –º–æ–¥–µ–ª–µ–π –æ—Å–Ω–∞—â–µ–Ω—ã –º–µ—Ç–∞–ª–ª–∏—á–µ—Å–∫–∏–º–∏Âà∑–∫–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æÊäì–≥–∞—é—Ç —á–∞—Å—Ç–∏—Ü—ã –≥—Ä—è–∑–∏ –∏ —à–µ—Ä–æ—Ö–æ–≤–∞—Ç–æ—Å—Ç–µ–π. –î—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∞–±—Ä–∞–∑–∏–≤–Ω—ã–µ –Ω–∞–∫–æ–Ω–µ—Ü, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–º–æ–≥–∞—é—Ç —É–¥–∞–ª–∏—Ç—å –≥–ª—É–±–æ–∫–∏–µ —É–≥–ª—É–±–ª–µ–Ω–∏—è. –í—ã–±–∏—Ä–∞–π—Ç–µ –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –≤–∞—à–µ–º—É —Ç–∏–ø—É –æ–±—É–≤–∏ –∏ —á–∞—Å—Ç–æ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.\n",
      "\n",
      "### 2. –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å\n",
      "–≠–ª–µ–∫—Ç—Ä–æ—á–∏—Å—Ç–∏–ª–∫–∏ –ø–æ–¥–∫–ª—é—á–∞—é—Ç—Å—è –∫ –∏—Å—Ç–æ—á–Ω–∏–∫—É —Ç–æ–∫–∞, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –∏—Ö –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º, –Ω–æ –∏ –±–æ–ª–µ–µ –±–µ–∑–æ–ø–∞—Å–Ω—ã–º. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏, –∏–º–µ–µ—Ç –∑–∞—â–∏—Ç–Ω—ã–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –ø—Ä–æ—Ç–∏–≤ –∫–æ—Ä–æ—Ç–∫–æ–≥–æ –∑–∞–º—ã–∫–∞–Ω–∏—è –∏ –ø–µ—Ä–µ–≥—Ä—É–∑–∫–∏. –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ, –µ—Å—Ç—å –ª–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≤—ã–∫–ª—é—á–∫–∞ –ø—Ä–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.\n",
      "\n",
      "### 3. –î–æ–ª–≥–æ–≤–µ—á–Ω–æ—Å—Ç—å\n",
      "–ö–∞—á–µ—Å—Ç–≤–æ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –≤ —ç–ª–µ–∫—Ç—Ä–æ—á–∏—Å—Ç–∏–ª–∫–∞—Ö –≤–∞–∂–Ω–æ –¥–ª—è –∏—Ö –¥–æ–ª–≥–æ–≤–µ—á–Ω–æ—Å—Ç–∏. –ú–µ—Ç–∞–ª–ª–∏—á–µ—Å–∫–∏–µ —á–∞—Å—Ç–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø—Ä–æ—á–Ω—ã–º–∏, –∞ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∏–µ –∫–æ–Ω—Ç–∞–∫—Ç—ã ‚Äî –Ω–∞–¥–µ–∂–Ω—ã–º–∏. –í—ã–±–∏—Ä–∞–π—Ç–µ –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –∏–º–µ—é—Ç –≥–∞—Ä–∞–Ω—Ç–∏—é –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –¥–æ–ª–≥–æ–≤–µ—á–Ω–æ—Å—Ç—å, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–æ–±–ª–µ–º —Å –∏–∑–Ω–æ—Å–æ–º –∏–ª–∏ –Ω–µ–∏—Å–ø—Ä–∞–≤–Ω–æ—Å—Ç—è–º–∏ –≤ –±—É–¥—É—â–µ–º.\n",
      "\n",
      "### 4. –°—Ç–æ–∏–º–æ—Å—Ç—å\n",
      "–≠–ª–µ–∫—Ç—Ä–æ—á–∏—Å—Ç–∏–ª–∫–∏ –¥–ª—è –æ–±—É–≤–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç—Å—è –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ü–µ–Ω–æ–≤—ã—Ö —É—Ä–æ–≤–Ω—è—Ö. –í—ã–±–∏—Ä–∞–π—Ç–µ –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –≤–∞—à–µ–º—É –±—é–¥–∂–µ—Ç—É –∏ –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç—è–º. –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ —Ä–∞–∑–Ω–æ—Å—è—á–µ–Ω–∏–µ –∏–ª–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Å–º–µ–Ω–∞ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –∏—Ö –±–æ–ª–µ–µ —É–¥–æ–±–Ω—ã–º–∏, –Ω–æ –∏ –±–æ–ª–µ–µ –¥–æ—Ä–æ–≥–∏–º–∏.\n",
      "\n",
      "### 5. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏\n",
      "–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —ç–ª–µ–∫—Ç—Ä–æ—á–∏—Å—Ç–∏–ª–∫–∏ —á–∞—Å—Ç–æ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω—ã —Ä–∞–∑–Ω–æ—Å—è—á–µ–Ω–∏–µ–º, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏—Ö –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –æ–±—É–≤–∏. –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –º–æ–¥–µ–ª–∏ –º–æ–∂–Ω–æ –ø–æ–¥–∫–ª—é—á–∞—Ç—å –∫ –∏—Å—Ç–æ—á–Ω–∏–∫—É —Ç–æ–∫–∞ —á–µ—Ä–µ–∑ USB, —á—Ç–æ —É–¥–æ–±–Ω–æ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ —Ä–∞–∑–Ω—ã—Ö –ø–æ–º–µ—â–µ–Ω–∏—è—Ö. –≠—Ç–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω—ã, –Ω–æ –∏—Ö —Å—Ç–æ–∏–º–æ—Å—Ç—å –º–æ–∂–µ—Ç –±—ã—Ç—å –≤–∫–ª—é—á–µ–Ω–∞ –≤ —Ü–µ–Ω—É –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥–µ–ª–∏.\n",
      "\n",
      "### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≤—ã–±–æ—Ä—É\n",
      "–î–ª—è –≤—ã–±–æ—Ä–∞ –ø–æ–¥—Ö–æ–¥—è—â–µ–π –º–æ–¥–µ–ª–∏ —ç–ª–µ–∫—Ç—Ä–æ—á–∏—Å—Ç–∏–ª–∫–∏ –¥–ª—è –æ–±—É–≤–∏ –≤–∞–∂–Ω–æ —É—á–∏—Ç—ã–≤–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–µ —Ñ–∞–∫—Ç–æ—Ä—ã:\n",
      "- –ß–∞—Å—Ç–æ—Ç—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è: –µ—Å–ª–∏ –≤—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ –æ–±—É–≤—å —á–∞—Å—Ç–æ, –≤—ã–±–∏—Ä–∞–π—Ç–µ –º–æ–¥–µ–ª–∏ —Å –±–æ–ª–µ–µÈïø–æ–π –≥–∞—Ä–∞–Ω—Ç–∏–µ–π –∏ –±–æ–ª–µ–µ –º–æ—â–Ω—ã–º–∏ —Ñ—É–Ω–∫—Ü–∏—è–º–∏.\n",
      "- –†–∞–∑–º–µ—Ä –æ–±—É–≤–∏: –Ω–µ–±–æ–ª—å—à–∏–µ –º–æ–¥–µ–ª–∏ –ª—É—á—à–µ –ø–æ–¥—Ö–æ–¥—è—Ç –¥–ª—è –¥–µ—Ç—Å–∫–∏—Ö —Å–∞–ø–æ–∫, –∞ –∫—Ä—É–ø–Ω—ã–µ ‚Äî –¥–ª—è –≤–∑—Ä–æ—Å–ª–æ–π –æ–±—É–≤–∏.\n",
      "- –õ–∏—á–Ω—ã–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è: –µ—Å–ª–∏ –≤—ã —Ü–µ–Ω–∏—Ç–µ —Å—Ç–∏–ª—å –∏–ª–∏ –¥–∏–∑–∞–π–Ω, –≤—ã–±–∏—Ä–∞–π—Ç–µ –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –≤–∞—à–µ–º—É –∏–Ω—Ç–µ—Ä—å–æ—Ä—É.\n",
      "\n",
      "–≠–ª–µ–∫—Ç—Ä–æ—á–∏—Å—Ç–∏–ª–∫–∏ –¥–ª—è –æ–±—É–≤–∏ ‚Äî —ç—Ç–æ –ø–æ–ª–µ–∑–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ø—Ä–æ—Å—Ç–∏—Ç—å –≤–∞—à—É –∂–∏–∑–Ω—å, –µ—Å–ª–∏ –≤—ã–±—Ä–∞–Ω—ã –ø—Ä–∞–≤–∏–ª—å–Ω–æ. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤—ã —É—á–∏—Ç—ã–≤–∞–µ—Ç –≤—Å–µ —Å–≤–æ–∏ –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –ø—Ä–∏ –≤—ã–±–æ—Ä–µ –º–æ–¥–µ–ª–∏, —á—Ç–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ.<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"–ù–∞–ø–∏—Å–∞—Ç—å —Ç–µ–∫—Å—Ç, –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –æ—Ü–µ–Ω–∏–≤–∞—é—â–∏–π —ç–ª–µ–∫—Ç—Ä–æ—á–∏—Å—Ç–∏–ª–∫–∏ –¥–ª—è –æ–±—É–≤–∏.\"\"\"\n",
    "\n",
    "FastLanguageModel.for_inference(model)  # Unsloth has 2x faster inference!\n",
    "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1200,\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "print(response[0].split(\"</think>\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adbda2f-1193-4064-8b61-09d3b1526b51",
   "metadata": {},
   "source": [
    "## –û–±—É—á–∞–µ–º DeepSeek R1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b75ee52-58fa-4a7b-871f-8c46ee7aad41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.3.19 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LoRA\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  \n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,  \n",
    "    bias=\"none\",  \n",
    "    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
    "    random_state=3407,\n",
    "    use_rslora=False,  \n",
    "    loftq_config=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3455f87-01f1-4f4e-b65a-2611be35df9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['tensorboard', '--logdir', 'outputs']>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–∏—Å–∞ TensorBoard\n",
    "subprocess.Popen([\"tensorboard\", \"--logdir\", \"outputs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86e21ddd-1769-4f85-b008-55c40d6e21e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "Write a response that appropriately completes the request. \n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<think>\n",
    "\n",
    "</think>\n",
    "{}\"\"\"\n",
    "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    instruction = \"–í—ã —è–≤–ª—è–µ—Ç–µ—Å—å –ø–∏—Å–∞—Ç–µ–ª–µ–º –∫–Ω–∏–≥ –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –±–ª–æ–≥–≥–µ—Ä–æ–º –∂–∏–≤–æ–≥–æ –∂—É—Ä–Ω–∞–ª–∞, —Å–æ —Å–≤–æ–∏–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–º —Å—Ç–∏–ª–µ–º –Ω–∞–ø–∏—Å–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –Ω–∞–ø–∏—à–∏—Ç–µ —Ç–µ–∫—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –ø–æ —Å–ª–µ–¥—É—é—â–µ–º—É –∑–∞–¥–∞–Ω–∏—é.\"\n",
    "    inputs       = examples[\"Prompt\"]\n",
    "    outputs      = examples[\"Message\"]\n",
    "    texts = []\n",
    "    for input, output in zip(inputs, outputs):\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "513e7f1f-3693-4e97-b661-078595128886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d67c03819bea4dc787e4004a47b7201b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5436 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Below is an instruction that describes a task, paired with an input that provides further context. \\nWrite a response that appropriately completes the request. \\nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\\n\\n### Instruction:\\n–í—ã —è–≤–ª—è–µ—Ç–µ—Å—å –ø–∏—Å–∞—Ç–µ–ª–µ–º –∫–Ω–∏–≥ –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –±–ª–æ–≥–≥–µ—Ä–æ–º –∂–∏–≤–æ–≥–æ –∂—É—Ä–Ω–∞–ª–∞, —Å–æ —Å–≤–æ–∏–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–º —Å—Ç–∏–ª–µ–º –Ω–∞–ø–∏—Å–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –Ω–∞–ø–∏—à–∏—Ç–µ —Ç–µ–∫—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –ø–æ —Å–ª–µ–¥—É—é—â–µ–º—É –∑–∞–¥–∞–Ω–∏—é.\\n\\n### Question:\\n–ù–∞–ø–∏—à–∏—Ç–µ —Ç–µ–∫—Å—Ç, –æ–±—ä—è—Å–Ω—è—é—â–∏–π —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É —Ç–µ—Ä–º–∏–Ω–∞–º–∏ \"–∑–∞–≥—Ä—É–∑–∫–∞\" –∏ \"—Å–∫–∞—á–∏–≤–∞–Ω–∏–µ\" –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Ä–∞–±–æ—Ç—ã —Å –¥–∞–Ω–Ω—ã–º–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.\\n\\n### Response:\\n<think>\\n\\n</think>\\n–ó–∞–≥—Ä—É–∑–∫–∞ –∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ\\n–õ—é–±–æ–ø—ã—Ç–Ω–æ, —á—Ç–æ –≤ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –ø—Ä–æ—Ü–µ—Å—Å –ø–µ—Ä–µ–¥–∞—á–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–∞—Ä—É–∂—É (–Ω–∞ —Å–µ—Ä–≤–µ—Ä –∏–ª–∏ –≤ –ø–∞–º—è—Ç—å) –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è \"–∑–∞–≥—Ä—É–∑–∫–∞\", –∞ –ø—Ä–æ—Ü–µ—Å—Å –ø–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑–≤–Ω–µ - \"—Å–∫–∞—á–∏–≤–∞–Ω–∏–µ\".\\n–ü—Ä–µ–∫—Ä–∞—Å–Ω—ã–π –ø—Ä–∏–º–µ—Ä: \"–°–∫–∞—á–∞—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º—ã –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ó–∞–≥—Ä—É–∑–∫–∞\".\\n–¢–æ –µ—Å—Ç—å, –Ω–∞—Ä—É–∂—É –º—ã –≥—Ä—É–∑–∏–º, —Å–æ–≤–µ—Ä—à–∞–µ–º –¥–µ–π—Å—Ç–≤–∏–µ —Å —á–µ–º-—Ç–æ —Ç–≤–µ—Ä–¥—ã–º. –ê –∫ —Å–µ–±–µ –º—ã —Å–∫–∞—á–∏–≤–∞–µ–º, —Ç–æ –µ—Å—Ç—å, —Å–æ–≤–µ—Ä—à–∞–µ–º –¥–µ–π—Å—Ç–≤–∏–µ —Å —á–µ–º-—Ç–æ –∂–∏–¥–∫–∏–º –∏ –±–µ—Å—Ñ–æ—Ä–º–µ–Ω–Ω—ã–º.<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "dataset = Dataset.from_pandas(pd.read_csv(\"datasets/Lebedev_messages_5436_with_prompt.csv\"))\n",
    "formatting_dataset = dataset.map(formatting_prompts_func, batched = True)\n",
    "formatting_dataset[\"text\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ee54246-b0c0-4df5-a087-98e410db0fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb7260d0d74546b9b9190bd9e1016ebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"]:   0%|          | 0/5436 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç—Ä–µ–Ω–µ—Ä–∞\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=formatting_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=1,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=8,\n",
    "        warmup_steps=10,\n",
    "        max_steps=60,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs/deepseek_r1\",\n",
    "        report_to=[\"tensorboard\"],\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebd3212a-b158-491b-9e6d-baed8f661d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4070 Ti SUPER.\n",
      "–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è 6.051 GB –∏–∑ 15.992 GB vRAM.\n"
     ]
    }
   ],
   "source": [
    "# –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–µ–∫—É—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É vRAM\n",
    "start_gpu_memory = display_vram_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23203167-4b6f-497f-aab5-2161db2861e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 5,436 | Num Epochs = 1 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 8 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 41,943,040/8,000,000,000 (0.52% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 05:49, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.802000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.852000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.646300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.611100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.526200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.487200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9aae9bdc-f6fb-4b75-8a2b-6db64a75cac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–û–±—É—á–µ–Ω–∏–µ –¥–ª–∏–ª–æ—Å—å 357.3391 —Å–µ–∫—É–Ω–¥ (5.96 –º–∏–Ω—É—Ç).\n",
      "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–æ—Å—å vRAM = 8.852 –ì–ë.\n",
      "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–æ—Å—å vRAM –¥–ª—è –æ–±—É—á–µ–Ω–∏—è = 2.801 –ì–ë.\n",
      "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–æ—Å—å vRAM –≤ % –æ—Ç –æ–±—â–µ–π –ø–∞–º—è—Ç–∏ = 55.353 %.\n",
      "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–æ—Å—å vRAM –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤ % –æ—Ç –æ–±—â–µ–π –ø–∞–º—è—Ç–∏ = 17.515 %.\n"
     ]
    }
   ],
   "source": [
    "# –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–∞–º—è—Ç–∏ –∏ –≤—Ä–µ–º–µ–Ω–∏\n",
    "display_vram_stats(\"after\", start_gpu_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467b8e9a-d747-4890-a8f9-3ba93ab47368",
   "metadata": {},
   "source": [
    "## –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å DeepSeek R1 –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3437cb2-e223-4931-8257-3545d10caa43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "–≠–ª–µ–∫—Ç—Ä–æ—á–∏—Å—Ç–∏–ª–∫–∏ –¥–ª—è –æ–±—É–≤–∏\n",
      "–≠–ª–µ–∫—Ç—Ä–æ—á–∏—Å—Ç–∏–ª–∫–∏ –¥–ª—è –æ–±—É–≤–∏ - —ç—Ç–æ —Ö—É–π–Ω—è. \n",
      "–ï—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å —ç–ª–µ–∫—Ç—Ä–æ—á–∏—Å—Ç–∏–ª–∫–∞ –¥–ª—è –æ–±—É–≤–∏, —Ç–æ –≤—ã —Å–µ–±–µ –ø–∏–∑–¥–µ—Ü –Ω–µ —á–∏—Å—Ç–∏—Ç–µ –æ–±—É–≤–∏. \n",
      "–í–æ—Ç —á—Ç–æ –±—É–¥–µ—Ç: –≤—ã –≤–∫–ª—é—á–∞–µ—Ç–µ —ç–ª–µ–∫—Ç—Ä–æ—á–∏—Å—Ç–∏–ª–∫—É –¥–ª—è –æ–±—É–≤–∏, –æ–Ω–∞ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è, –≤—ã –∫–ª–∞–¥–µ—Ç–µ –æ–±—É–≤–∏ –Ω–∞ –Ω–µ–µ, –æ–Ω–∞ –Ω–∞—Ö—É–π –Ω–µ —á–∏—Å—Ç–∏—Ç. \n",
      "–ê —ç—Ç–æ –ø–æ—Ç–æ–º—É, —á—Ç–æ —ç–ª–µ–∫—Ç—Ä–æ—á–∏—Å—Ç–∏–ª–∫–∞ –¥–ª—è –æ–±—É–≤–∏ –Ω–µ –º–æ–∂–µ—Ç –Ω–∏—á–µ–≥–æ —Å–¥–µ–ª–∞—Ç—å —Å –æ–±—É–≤–∏. \n",
      "–ê –µ—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ, —á—Ç–æ–±—ã –æ–±—É—è –±—ã–ª–∞ —á–∏—Å—Ç–æ–π, —Ç–æ –≤—ã –ø—Ä–æ—Å—Ç–æ —Å–Ω–∞—á–∞–ª–∞ –µ–µ –≤—ã–º–æ—á–∏—Ç–µ.<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\n"
     ]
    }
   ],
   "source": [
    "question = \"–ù–∞–ø–∏—Å–∞—Ç—å —Ç–µ–∫—Å—Ç, –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –æ—Ü–µ–Ω–∏–≤–∞—é—â–∏–π —ç–ª–µ–∫—Ç—Ä–æ—á–∏—Å—Ç–∏–ª–∫–∏ –¥–ª—è –æ–±—É–≤–∏.\"\n",
    "\n",
    "FastLanguageModel.for_inference(model)  # Unsloth has 2x faster inference!\n",
    "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1200,\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "print(response[0].split(\"</think>\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0c353e-baed-4192-8d97-a91927429197",
   "metadata": {},
   "source": [
    "## –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–¥–∞–ø—Ç–µ—Ä—ã LoRA –∏ –º–æ–¥–µ–ª—å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b48b55b4-d1b9-49ef-93a6-19940f0fe9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 75.13 out of 127.91 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|‚ñà‚ñà‚ñà‚ñà      | 13/32 [00:00<00:00, 20.92it/s]\n",
      "We will save to Disk and not RAM now.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:13<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Done.\n",
      "–û—á–∏—Å—Ç–∫–∞ VRAM...\n",
      "VRAM —É—Å–ø–µ—à–Ω–æ –æ—á–∏—â–µ–Ω–∞.\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"Lebedev_DeepSeek-R1\")\n",
    "tokenizer.save_pretrained(\"Lebedev_DeepSeek-R1\")\n",
    "model.save_pretrained_merged(\"Lebedev_DeepSeek-R1-finetune\", tokenizer)\n",
    "del model\n",
    "del tokenizer\n",
    "del inputs\n",
    "del outputs\n",
    "del trainer\n",
    "del response\n",
    "del trainer_stats\n",
    "del formatting_dataset\n",
    "clear_vram()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4d9ad9-6ed8-4db6-b5ec-6d38aec915ef",
   "metadata": {},
   "source": [
    "# Gemma 2\n",
    "## –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å Gemma 2 –¥–æ –æ–±—É—á–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3200850-c7e1-4f93-adb3-682c0d4e89b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Gemma2 patching. Transformers: 4.50.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4070 Ti SUPER. Num GPUs = 1. Max memory: 15.992 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/gemma-2-9b-it-bnb-4bit\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b25ea05d-ffa6-434c-8a98-1a4c92c791ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      "–í –º–µ—Ç—Ä–æ, –≤ –∞–≤—Ç–æ–±—É—Å–µ, –≤ –æ—á–µ—Ä–µ–¥–∏ –∑–∞ –∫–æ—Ñ–µ ‚Äì –º—ã –≤—Å–µ –≤—Ä–µ–º—è –≥–¥–µ-—Ç–æ —Ä—è–¥–æ–º –¥—Ä—É–≥ —Å –¥—Ä—É–≥–æ–º. –ò —ç—Ç–æ –∑–¥–æ—Ä–æ–≤–æ, –≤–µ–¥—å –æ–±—â–µ–Ω–∏–µ ‚Äì –æ—Å–Ω–æ–≤–∞ –∂–∏–∑–Ω–∏! –ù–æ –µ—Å—Ç—å –æ–¥–Ω–æ \"–Ω–æ\", –∫–æ—Ç–æ—Ä–æ–µ –Ω–µ —Ç–∞–∫ –ø—Ä–∏—è—Ç–Ω–æ ‚Äì –Ω–µ—Å–æ–±–ª—é–¥–µ–Ω–∏–µ –ª–∏—á–Ω–æ–π –≥–∏–≥–∏–µ–Ω—ã. \n",
      "\n",
      "–ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ: –≤—ã –µ–¥–µ—Ç–µ –≤ –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–Ω–æ–º –∞–≤—Ç–æ–±—É—Å–µ, –∞ —Ä—è–¥–æ–º —Å –≤–∞–º–∏ —á–µ–ª–æ–≤–µ–∫, –∫–æ—Ç–æ—Ä—ã–π –Ω–µ –º—ã–ª—Å—è —É–∂–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–Ω–µ–π. –ò–ª–∏ –≤—ã —Å—Ç–æ–∏—Ç–µ –≤ –æ—á–µ—Ä–µ–¥–∏, –∏ –∫—Ç–æ-—Ç–æ —Ä—è–¥–æ–º —Å –≤–∞–º–∏ –≥—Ä–æ–º–∫–æ —á–∏—Ö–∞–µ—Ç, –Ω–µ –ø—Ä–∏–∫—Ä—ã–≤–∞—è —Ä–æ—Ç. –ù–µ–ø—Ä–∏—è—Ç–Ω–æ, –ø—Ä–∞–≤–¥–∞? \n",
      "\n",
      "–í–∞–∂–Ω–æ –ø–æ–º–Ω–∏—Ç—å, —á—Ç–æ –ª–∏—á–Ω–∞—è –≥–∏–≥–∏–µ–Ω–∞ ‚Äì —ç—Ç–æ –Ω–µ —Ç–æ–ª—å–∫–æ –ø—Ä–æ –Ω–∞—Å —Å–∞–º–∏—Ö, –Ω–æ –∏ –ø—Ä–æ —Ç–µ—Ö, –∫—Ç–æ —Ä—è–¥–æ–º. –ú—ã –≤—Å–µ —Ö–æ—Ç–∏–º —á—É–≤—Å—Ç–≤–æ–≤–∞—Ç—å —Å–µ–±—è –∫–æ–º—Ñ–æ—Ä—Ç–Ω–æ –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ –≤ –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö. –ê —ç—Ç–æ –≤–æ–∑–º–æ–∂–Ω–æ —Ç–æ–ª—å–∫–æ —Ç–æ–≥–¥–∞, –∫–æ–≥–¥–∞ –∫–∞–∂–¥—ã–π –∏–∑ –Ω–∞—Å –∑–∞–±–æ—Ç–∏—Ç—Å—è –æ —Å–≤–æ–µ–π —á–∏—Å—Ç–æ—Ç–µ –∏ –ø–æ—Ä—è–¥–∫–µ. \n",
      "\n",
      "–¢–∞–∫ —á—Ç–æ –¥–∞–≤–∞–π—Ç–µ –±—ã—Ç—å –≤–µ–∂–ª–∏–≤—ã–º–∏ –¥—Ä—É–≥ –∫ –¥—Ä—É–≥—É –∏ —Å–ª–µ–¥–∏—Ç—å –∑–∞ —Å–≤–æ–µ–π –≥–∏–≥–∏–µ–Ω–æ–π! –í–µ–¥—å —ç—Ç–æ –Ω–µ—Å–ª–æ–∂–Ω–æ, –∞ –ø–æ–ª—å–∑–∞ –æ—Ç —ç—Ç–æ–≥–æ –æ–≥—Ä–æ–º–Ω–∞—è. \n",
      "\n",
      "\n",
      "<end_of_turn>\n",
      "<eos>\n"
     ]
    }
   ],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"–í—ã —è–≤–ª—è–µ—Ç–µ—Å—å –ø–∏—Å–∞—Ç–µ–ª–µ–º –∫–Ω–∏–≥ –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –±–ª–æ–≥–≥–µ—Ä–æ–º –∂–∏–≤–æ–≥–æ –∂—É—Ä–Ω–∞–ª–∞, —Å–æ —Å–≤–æ–∏–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–º —Å—Ç–∏–ª–µ–º –Ω–∞–ø–∏—Å–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –Ω–∞–ø–∏—à–∏—Ç–µ —Ç–µ–∫—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –ø–æ —Å–ª–µ–¥—É—é—â–µ–º—É –∑–∞–¥–∞–Ω–∏—é.\", # instruction\n",
    "        \"–ù–∞–ø–∏—Å–∞—Ç—å –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç, –æ–±—ä—è—Å–Ω—è—é—â–∏–π, –ø–æ—á–µ–º—É –≤–∞–∂–Ω–æ —Å–æ–±–ª—é–¥–∞—Ç—å –ø—Ä–∞–≤–∏–ª–∞ –ª–∏—á–Ω–æ–π –≥–∏–≥–∏–µ–Ω—ã –≤ –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö.\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=500,\n",
    "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "print(response[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22d0656-d0cd-43e7-9d47-78af912b12b9",
   "metadata": {},
   "source": [
    "## –û–±—É—á–∞–µ–º Gemma 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "705225aa-bdf5-4829-acaa-a7a8ffdabd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.3.19 patched 42 layers with 42 QKV layers, 42 O layers and 42 MLP layers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf9b6355a20c4f349401cb138c60fb1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5436 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'Message': '–ó–∞–≥—Ä—É–∑–∫–∞ –∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ\\n–õ—é–±–æ–ø—ã—Ç–Ω–æ, —á—Ç–æ –≤ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –ø—Ä–æ—Ü–µ—Å—Å –ø–µ—Ä–µ–¥–∞—á–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–∞—Ä—É–∂—É (–Ω–∞ —Å–µ—Ä–≤–µ—Ä –∏–ª–∏ –≤ –ø–∞–º—è—Ç—å) –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è \"–∑–∞–≥—Ä—É–∑–∫–∞\", –∞ –ø—Ä–æ—Ü–µ—Å—Å –ø–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑–≤–Ω–µ - \"—Å–∫–∞—á–∏–≤–∞–Ω–∏–µ\".\\n–ü—Ä–µ–∫—Ä–∞—Å–Ω—ã–π –ø—Ä–∏–º–µ—Ä: \"–°–∫–∞—á–∞—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º—ã –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ó–∞–≥—Ä—É–∑–∫–∞\".\\n–¢–æ –µ—Å—Ç—å, –Ω–∞—Ä—É–∂—É –º—ã –≥—Ä—É–∑–∏–º, —Å–æ–≤–µ—Ä—à–∞–µ–º –¥–µ–π—Å—Ç–≤–∏–µ —Å —á–µ–º-—Ç–æ —Ç–≤–µ—Ä–¥—ã–º. –ê –∫ —Å–µ–±–µ –º—ã —Å–∫–∞—á–∏–≤–∞–µ–º, —Ç–æ –µ—Å—Ç—å, —Å–æ–≤–µ—Ä—à–∞–µ–º –¥–µ–π—Å—Ç–≤–∏–µ —Å —á–µ–º-—Ç–æ –∂–∏–¥–∫–∏–º –∏ –±–µ—Å—Ñ–æ—Ä–º–µ–Ω–Ω—ã–º.',\n",
       " 'Prompt': '–ù–∞–ø–∏—à–∏—Ç–µ —Ç–µ–∫—Å—Ç, –æ–±—ä—è—Å–Ω—è—é—â–∏–π —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É —Ç–µ—Ä–º–∏–Ω–∞–º–∏ \"–∑–∞–≥—Ä—É–∑–∫–∞\" –∏ \"—Å–∫–∞—á–∏–≤–∞–Ω–∏–µ\" –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Ä–∞–±–æ—Ç—ã —Å –¥–∞–Ω–Ω—ã–º–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.',\n",
       " 'text': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n–í—ã —è–≤–ª—è–µ—Ç–µ—Å—å –ø–∏—Å–∞—Ç–µ–ª–µ–º –∫–Ω–∏–≥ –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –±–ª–æ–≥–≥–µ—Ä–æ–º –∂–∏–≤–æ–≥–æ –∂—É—Ä–Ω–∞–ª–∞, —Å–æ —Å–≤–æ–∏–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–º —Å—Ç–∏–ª–µ–º –Ω–∞–ø–∏—Å–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –Ω–∞–ø–∏—à–∏—Ç–µ —Ç–µ–∫—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –ø–æ —Å–ª–µ–¥—É—é—â–µ–º—É –∑–∞–¥–∞–Ω–∏—é.\\n\\n### Input:\\n–ù–∞–ø–∏—à–∏—Ç–µ —Ç–µ–∫—Å—Ç, –æ–±—ä—è—Å–Ω—è—é—â–∏–π —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É —Ç–µ—Ä–º–∏–Ω–∞–º–∏ \"–∑–∞–≥—Ä—É–∑–∫–∞\" –∏ \"—Å–∫–∞—á–∏–≤–∞–Ω–∏–µ\" –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Ä–∞–±–æ—Ç—ã —Å –¥–∞–Ω–Ω—ã–º–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.\\n\\n### Response:\\n–ó–∞–≥—Ä—É–∑–∫–∞ –∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ\\n–õ—é–±–æ–ø—ã—Ç–Ω–æ, —á—Ç–æ –≤ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –ø—Ä–æ—Ü–µ—Å—Å –ø–µ—Ä–µ–¥–∞—á–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–∞—Ä—É–∂—É (–Ω–∞ —Å–µ—Ä–≤–µ—Ä –∏–ª–∏ –≤ –ø–∞–º—è—Ç—å) –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è \"–∑–∞–≥—Ä—É–∑–∫–∞\", –∞ –ø—Ä–æ—Ü–µ—Å—Å –ø–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑–≤–Ω–µ - \"—Å–∫–∞—á–∏–≤–∞–Ω–∏–µ\".\\n–ü—Ä–µ–∫—Ä–∞—Å–Ω—ã–π –ø—Ä–∏–º–µ—Ä: \"–°–∫–∞—á–∞—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º—ã –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ó–∞–≥—Ä—É–∑–∫–∞\".\\n–¢–æ –µ—Å—Ç—å, –Ω–∞—Ä—É–∂—É –º—ã –≥—Ä—É–∑–∏–º, —Å–æ–≤–µ—Ä—à–∞–µ–º –¥–µ–π—Å—Ç–≤–∏–µ —Å —á–µ–º-—Ç–æ —Ç–≤–µ—Ä–¥—ã–º. –ê –∫ —Å–µ–±–µ –º—ã —Å–∫–∞—á–∏–≤–∞–µ–º, —Ç–æ –µ—Å—Ç—å, —Å–æ–≤–µ—Ä—à–∞–µ–º –¥–µ–π—Å—Ç–≤–∏–µ —Å —á–µ–º-—Ç–æ –∂–∏–¥–∫–∏–º –∏ –±–µ—Å—Ñ–æ—Ä–º–µ–Ω–Ω—ã–º.<eos>'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")\n",
    "\n",
    "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
    "EOS_TOKEN = tokenizer.eos_token  # –î–æ–±–∞–≤–ª—è–µ–º EOS-—Ç–æ–∫–µ–Ω (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ formatting_prompts_func)\n",
    "\n",
    "formatting_dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "formatting_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23ca469f-c3a3-494e-a115-2dcf62014ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bcc6af4ac0f4ad29992a0a09178a0ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"]:   0%|          | 0/5436 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç—Ä–µ–Ω–µ—Ä–∞ –¥–ª—è Gemma2\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=formatting_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 1,\n",
    "    packing = False,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 10,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir=\"outputs/gemma2\",\n",
    "        report_to=[\"tensorboard\"],\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6833381-cb55-469d-b7c4-5fc5295d8013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4070 Ti SUPER.\n",
      "–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è 13.717 GB –∏–∑ 15.992 GB vRAM.\n"
     ]
    }
   ],
   "source": [
    "# –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–µ–∫—É—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É vRAM\n",
    "start_gpu_memory = display_vram_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82a23be0-e8b8-45df-a583-51c65e9f5efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 5,436 | Num Epochs = 1 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 54,018,048/9,000,000,000 (0.60% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 03:08, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.333700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.683800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.665800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.589600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.573700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.644000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e43f9f4c-fc3b-41ae-b3c8-da418ef08d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–û–±—É—á–µ–Ω–∏–µ –¥–ª–∏–ª–æ—Å—å 192.2821 —Å–µ–∫—É–Ω–¥ (3.2 –º–∏–Ω—É—Ç).\n",
      "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–æ—Å—å vRAM = 13.717 –ì–ë.\n",
      "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–æ—Å—å vRAM –¥–ª—è –æ–±—É—á–µ–Ω–∏—è = 0.0 –ì–ë.\n",
      "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–æ—Å—å vRAM –≤ % –æ—Ç –æ–±—â–µ–π –ø–∞–º—è—Ç–∏ = 85.774 %.\n",
      "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–æ—Å—å vRAM –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤ % –æ—Ç –æ–±—â–µ–π –ø–∞–º—è—Ç–∏ = 0.0 %.\n"
     ]
    }
   ],
   "source": [
    "# –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–∞–º—è—Ç–∏ –∏ –≤—Ä–µ–º–µ–Ω–∏\n",
    "display_vram_stats(\"after\", start_gpu_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0070f352-226f-429c-8f48-3dff78b6ba0c",
   "metadata": {},
   "source": [
    "## –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å Gemma 2 –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e26ea899-c5fd-4263-9b4b-1ecd24acc7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "–õ–∏—á–Ω–∞—è –≥–∏–≥–∏–µ–Ω–∞\n",
      "–í –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö –≤–∞–∂–Ω–æ —Å–æ–±–ª—é–¥–∞—Ç—å –ª–∏—á–Ω—É—é –≥–∏–≥–∏–µ–Ω—É. –≠—Ç–æ –∫–∞—Å–∞–µ—Ç—Å—è –Ω–µ —Ç–æ–ª—å–∫–æ –º—ã—Ç—å—è —Ä—É–∫, –Ω–æ –∏ –¥—Ä—É–≥–∏—Ö –∞—Å–ø–µ–∫—Ç–æ–≤. –ù–∞–ø—Ä–∏–º–µ—Ä, –Ω–µ —Å—Ç–æ–∏—Ç –µ—Å—Ç—å –≤ –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–µ, —Ç–∞–∫ –∫–∞–∫ —ç—Ç–æ –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –ø–æ–ø–∞–¥–∞–Ω–∏—é –∫—Ä–æ—à–µ–∫ –Ω–∞ —Å–∏–¥–µ–Ω—å–µ –∏ –≤ –≤–æ–∑–¥—É—Ö. –¢–∞–∫–∂–µ –≤–∞–∂–Ω–æ –Ω–µ —Ä–∞–∑–≥–æ–≤–∞—Ä–∏–≤–∞—Ç—å –ø–æ —Ç–µ–ª–µ—Ñ–æ–Ω—É –≤ –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–µ, —Ç–∞–∫ –∫–∞–∫ —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–ø—Ä–∏—è—Ç–Ω–æ –¥–ª—è –¥—Ä—É–≥–∏—Ö –ø–∞—Å—Å–∞–∂–∏—Ä–æ–≤.\n",
      "–í–∞–∂–Ω–æ –ø–æ–º–Ω–∏—Ç—å, —á—Ç–æ –ª–∏—á–Ω–∞—è –≥–∏–≥–∏–µ–Ω–∞ - —ç—Ç–æ –Ω–µ —Ç–æ–ª—å–∫–æ –∑–∞–±–æ—Ç–∞ –æ —Å–µ–±–µ, –Ω–æ –∏ –∑–∞–±–æ—Ç–∞ –æ –¥—Ä—É–≥–∏—Ö –ª—é–¥—è—Ö.<eos>\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"–í—ã —è–≤–ª—è–µ—Ç–µ—Å—å –ø–∏—Å–∞—Ç–µ–ª–µ–º –∫–Ω–∏–≥ –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –±–ª–æ–≥–≥–µ—Ä–æ–º –∂–∏–≤–æ–≥–æ –∂—É—Ä–Ω–∞–ª–∞, —Å–æ —Å–≤–æ–∏–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–º —Å—Ç–∏–ª–µ–º –Ω–∞–ø–∏—Å–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –Ω–∞–ø–∏—à–∏—Ç–µ —Ç–µ–∫—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –ø–æ —Å–ª–µ–¥—É—é—â–µ–º—É –∑–∞–¥–∞–Ω–∏—é.\", # instruction\n",
    "        \"–ù–∞–ø–∏—Å–∞—Ç—å —Ç–µ–∫—Å—Ç, –æ–±—ä—è—Å–Ω—è—é—â–∏–π, –ø–æ—á–µ–º—É –≤–∞–∂–Ω–æ —Å–æ–±–ª—é–¥–∞—Ç—å –ø—Ä–∞–≤–∏–ª–∞ –ª–∏—á–Ω–æ–π –≥–∏–≥–∏–µ–Ω—ã –≤ –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö.\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=500,\n",
    "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "print(response[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3cadc4-d96d-4b2e-91b9-eebbf9f52cd9",
   "metadata": {},
   "source": [
    "## –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–¥–∞–ø—Ç–µ—Ä—ã LoRA –∏ –º–æ–¥–µ–ª—å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94f89f12-86a6-4526-9cd2-ebe5a023ec0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 74.77 out of 127.91 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42/42 [00:21<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Done.\n",
      "–û—á–∏—Å—Ç–∫–∞ VRAM...\n",
      "VRAM —É—Å–ø–µ—à–Ω–æ –æ—á–∏—â–µ–Ω–∞.\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"Lebedev_Gemma2\")\n",
    "tokenizer.save_pretrained(\"Lebedev_Gemma2\")\n",
    "model.save_pretrained_merged(\"Lebedev_Gemma2-finetune\", tokenizer)\n",
    "del model\n",
    "del tokenizer\n",
    "del inputs\n",
    "del outputs\n",
    "del trainer\n",
    "del response\n",
    "del trainer_stats\n",
    "del formatting_dataset\n",
    "clear_vram()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d837cee9-258d-4321-bfaa-4a9181cdb602",
   "metadata": {},
   "source": [
    "# Llama 3.1\n",
    "## –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å Llama 3.1 –¥–æ –æ–±—É—á–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd0902e6-efc0-4b79-986b-204a0f39d32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.50.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4070 Ti SUPER. Num GPUs = 1. Max memory: 15.992 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Meta-Llama-3.1-8B-Instruct-unsloth-bnb-4bit\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ab682fc-86c1-480f-afef-2ffb63408e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "–î–Ω–µ–≤–Ω–∏–∫.\n",
      "\n",
      "–ó–¥–æ—Ä–æ–≤—å–µ –Ω–µ —Ç–æ–ª—å–∫–æ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–µ, –Ω–æ –∏ –≥–∏–≥–∏–µ–Ω–∏—á–µ—Å–∫–æ–µ. –ß—Ç–æ–±—ã –∑–∞—â–∏—Ç–∏—Ç—å —Å–≤–æ–∏—Ö –∂–µ —Å–µ–±—è, –∞ —Ç–∞–∫–∂–µ –æ–∫—Ä—É–∂–∞—é—â–∏—Ö –ª—é–¥–µ–π –æ—Ç —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω—Ñ–µ–∫—Ü–∏–π, –≤–∞–∂–Ω–æ —Å–æ–±–ª—é–¥–∞—Ç—å –ø—Ä–æ—Å—Ç–µ–π—à–∏–µ –Ω–æ—Ä–º—ã –ª–∏—á–Ω–æ–π –≥–∏–≥–∏–µ–Ω—ã. –î–µ–ª–∞–π—Ç–µ —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ. –ü—Ä–µ–∂–¥–µ –≤—Å–µ–≥–æ –≤ –º–µ—Å—Ç–∞—Ö —Å –±–æ–ª—å—à–æ–π –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é - –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ç—É–∞–ª–µ—Ç—ã –≤ —à–∫–æ–ª–∞—Ö, —Ç–æ—Ä–≥–æ–≤—ã—Ö —Ü–µ–Ω—Ç—Ä–∞—Ö –∏–ª–∏ –Ω–∞ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–µ.\n",
      "–î–µ–ª–∞–π—Ç–µ —ç—Ç–æ –≤ —Ç–∞–∫–∏—Ö –º–µ—Å—Ç–∞—Ö. \n",
      "–ù–∞–ø—Ä–∏–º–µ—Ä –≤ –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö: –Ω–∞ –º–∞—Å—Å–æ–≤—ã—Ö —Å–æ–±—ã—Ç–∏—è—Ö. –°–ª–µ–¥–∏—Ç–µ –∑–∞ —Å–æ–±–æ–π –≤–æ –≤—Ä–µ–º—è —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –Ω–∞–≥—Ä—É–∑–æ–∫ –∏ –ø–æ—Å–ª–µ. –ü–æ—Å–ª–µ —Ç–æ–≥–æ, –∫–∞–∫ –í—ã –±—ã–ª–∏ –≤ –º–µ—Å—Ç–∞—Ö –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏. –≠—Ç–æ –æ—á–µ–Ω—å –ø—Ä–æ—Å—Ç–æ. –£–º–Ω—ã–π —Å–ø–æ—Å–æ–± —Å–¥–µ—Ä–∂–∏–≤–∞–Ω–∏—è —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω—Ñ–µ–∫—Ü–∏–∏, –∏ –≤ –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ —ç—Ç–æ –≤–∞—à –¥–æ–ª–≥. –í—Å–µ–≥–¥–∞ –ø–æ–º–Ω–∏—Ç–µ –æ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ª–∏—á–Ω–æ–π –≥–∏–≥–∏–µ–Ω—ã. –ü—Ä–∏–º–µ–Ω—è–π—Ç–µ –ø—Ä–∏–Ω—Ü–∏–ø \"—á–µ–ª–æ–≤–µ–∫-–æ–∫—Ä—É–∂–∞—é—â–∞—è —Å—Ä–µ–¥–∞\" - –í—ã –≤—Å–µ-—Ç–∞–∫–∏ –Ω–∞—Ö–æ–¥–∏—Ç–µ–µ—Å—å –≤ –Ω–µ–π. \n",
      "\n",
      "### –í–Ω–∏–º–∞–Ω–∏–µ:\n",
      "–ï—Å–ª–∏ –≤ –í–∞—à–µ–π —Å—Ç—Ä–∞–Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∑–∞–∫–æ–Ω–Ω–∞—è –Ω–æ—Ä–º–∞ –ª–∏—á–Ω–æ–π –≥–∏–≥–∏–µ–Ω—ã, –æ–ø–∏—Å–∞–Ω–Ω–∞—è –≤ —Ç–µ–∫—Å—Ç–µ –Ω–µ —Å—Ç–æ–∏—Ç –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ –Ω–∏—Ö –ø—Ä–∏ –Ω–∞–ø–∏—Å–∞–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞. –û–ø–∏—Å–∞–Ω–∏–µ –Ω–æ—Ä–º –ª–∏—á–Ω–æ–π –≥–∏–≥–∏–µ–Ω—ã –º–æ–∂–µ—Ç –≤–∫–ª—é—á–∞—Ç—å –≤ —Å–µ–±—è —ç–ª–µ–º–µ–Ω—Ç—ã –≥–∏–≥–∏–µ–Ω–∏–∫–∏ –∏ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Å–ª—É—á–∞–µ–≤ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–π.\n",
      "–≠—Ç–æ—Ç —Ç–µ–∫—Å—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –ø—Ä–æ—Å—Ç–æ–π —Ä—É–∫–æ–≤–æ–¥—è—â–∏–π —Ç–µ–∫—Å—Ç –ø–æ —Ç–µ–º–µ. –û—Å–Ω–æ–≤–Ω–∞—è —Ü–µ–ª—å: –ø—Ä–∏–≤–ª–µ—á—å –≤–Ω–∏–º–∞–Ω–∏–µ —á–∏—Ç–∞—Ç–µ–ª–µ–π –Ω–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ª–∏—á–Ω–æ–π –≥–∏–≥–∏–µ–Ω—ã. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –≥–∏–≥–∏–µ–Ω–∏—á–µ—Å–∫–∏–π –æ–±—Ä–∞–∑ –¥–µ–π—Å—Ç–≤–∏–π –≤ –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö, –∫–æ—Ç–æ—Ä–∞—è —É–∂–µ —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–∞ –≤ –ø—Ä–æ—Å—Ç–æ–º —Ç–æ–Ω–µ. –ß—Ç–æ–±—ã –ø—Ä–æ—á–∏—Ç–∞—Ç—å –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω—É—é –≥–∏–≥–∏–µ–Ω–∏—á–µ—Å–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –ø—Ä–æ—Å—Ç–æ –∫–ª–∏–∫–Ω–∏—Ç–µ –∑–¥–µ—Å—å.\n",
      "–°–ª–µ–¥—É—è –ø—Ä–æ—Å—Ç–æ–º—É –ø—Ä–∏–Ω—Ü–∏–ø—É - \"—á–µ–ª–æ–≤–µ–∫ - —Å—Ä–µ–¥–∞\", –∫–∞–∂–¥—ã–π —á–µ–ª–æ–≤–µ–∫ –∏–º–µ–µ—Ç –±–æ–ª—å—à–æ–µ –≤–ª–∏—è–Ω–∏–µ –Ω–∞ –∑–¥–æ—Ä–æ–≤—å–µ –æ–∫—Ä—É–∂–∞—é—â–µ–π —Å—Ä–µ–¥—ã. –ß—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –±–∞–ª–∞–Ω—Å –∏ –±–ª–∞–≥–æ–ø–æ–ª—É—á–∏–µ –æ–∫—Ä—É–∂–∞—é—â–µ–π —Å—Ä–µ–¥—ã, –≤–∞–∂–Ω—É—é —Ä–æ–ª—å –∏–≥—Ä–∞—é—Ç –µ–∂–µ–¥–Ω–µ–≤–Ω–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞ –∏ –∑–¥–æ—Ä–æ–≤—ã–µ –Ω–∞–≤—ã–∫–∏. –ú—ã –Ω–∞–¥–µ–µ–º—Å—è, —á—Ç–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ —Å–æ–≤–µ—Ç—ã –±—ã–ª–∏ –ø–æ–ª–µ–∑–Ω—ã –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º–∏ –¥–ª—è –≤–∞—Å. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–ø–ª–∞–Ω–∏—Ä—É–π—Ç–µ –ø—Ä–æ—á–∏—Ç–∞—Ç—å –Ω–∞—à–µ —Å–ª–µ–¥—É—é—â–µ–µ —Å–æ–≤–º–µ—Å—Ç–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"–í—ã —è–≤–ª—è–µ—Ç–µ—Å—å –ø–∏—Å–∞—Ç–µ–ª–µ–º –∫–Ω–∏–≥ –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –±–ª–æ–≥–≥–µ—Ä–æ–º –∂–∏–≤–æ–≥–æ –∂—É—Ä–Ω–∞–ª–∞, —Å–æ —Å–≤–æ–∏–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–º —Å—Ç–∏–ª–µ–º –Ω–∞–ø–∏—Å–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –Ω–∞–ø–∏—à–∏—Ç–µ —Ç–µ–∫—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –ø–æ —Å–ª–µ–¥—É—é—â–µ–º—É –∑–∞–¥–∞–Ω–∏—é.\", # instruction\n",
    "        \"–ù–∞–ø–∏—Å–∞—Ç—å –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç, –æ–±—ä—è—Å–Ω—è—é—â–∏–π, –ø–æ—á–µ–º—É –≤–∞–∂–Ω–æ —Å–æ–±–ª—é–¥–∞—Ç—å –ø—Ä–∞–≤–∏–ª–∞ –ª–∏—á–Ω–æ–π –≥–∏–≥–∏–µ–Ω—ã –≤ –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö.\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=500,\n",
    "    temperature = 1.5, min_p = 0.1,\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "print(response[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e55075-2206-484b-a66c-e7cb5fbc0701",
   "metadata": {},
   "source": [
    "## –û–±—É—á–∞–µ–º Llama 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f965ff51-0202-4a47-8f79-fecf3f55fb0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a22c37112304dfd82b278fb991b9793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5436 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'Message': '–ó–∞–≥—Ä—É–∑–∫–∞ –∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ\\n–õ—é–±–æ–ø—ã—Ç–Ω–æ, —á—Ç–æ –≤ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –ø—Ä–æ—Ü–µ—Å—Å –ø–µ—Ä–µ–¥–∞—á–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–∞—Ä—É–∂—É (–Ω–∞ —Å–µ—Ä–≤–µ—Ä –∏–ª–∏ –≤ –ø–∞–º—è—Ç—å) –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è \"–∑–∞–≥—Ä—É–∑–∫–∞\", –∞ –ø—Ä–æ—Ü–µ—Å—Å –ø–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑–≤–Ω–µ - \"—Å–∫–∞—á–∏–≤–∞–Ω–∏–µ\".\\n–ü—Ä–µ–∫—Ä–∞—Å–Ω—ã–π –ø—Ä–∏–º–µ—Ä: \"–°–∫–∞—á–∞—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º—ã –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ó–∞–≥—Ä—É–∑–∫–∞\".\\n–¢–æ –µ—Å—Ç—å, –Ω–∞—Ä—É–∂—É –º—ã –≥—Ä—É–∑–∏–º, —Å–æ–≤–µ—Ä—à–∞–µ–º –¥–µ–π—Å—Ç–≤–∏–µ —Å —á–µ–º-—Ç–æ —Ç–≤–µ—Ä–¥—ã–º. –ê –∫ —Å–µ–±–µ –º—ã —Å–∫–∞—á–∏–≤–∞–µ–º, —Ç–æ –µ—Å—Ç—å, —Å–æ–≤–µ—Ä—à–∞–µ–º –¥–µ–π—Å—Ç–≤–∏–µ —Å —á–µ–º-—Ç–æ –∂–∏–¥–∫–∏–º –∏ –±–µ—Å—Ñ–æ—Ä–º–µ–Ω–Ω—ã–º.',\n",
       " 'Prompt': '–ù–∞–ø–∏—à–∏—Ç–µ —Ç–µ–∫—Å—Ç, –æ–±—ä—è—Å–Ω—è—é—â–∏–π —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É —Ç–µ—Ä–º–∏–Ω–∞–º–∏ \"–∑–∞–≥—Ä—É–∑–∫–∞\" –∏ \"—Å–∫–∞—á–∏–≤–∞–Ω–∏–µ\" –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Ä–∞–±–æ—Ç—ã —Å –¥–∞–Ω–Ω—ã–º–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.',\n",
       " 'text': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n–í—ã —è–≤–ª—è–µ—Ç–µ—Å—å –ø–∏—Å–∞—Ç–µ–ª–µ–º –∫–Ω–∏–≥ –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –±–ª–æ–≥–≥–µ—Ä–æ–º –∂–∏–≤–æ–≥–æ –∂—É—Ä–Ω–∞–ª–∞, —Å–æ —Å–≤–æ–∏–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–º —Å—Ç–∏–ª–µ–º –Ω–∞–ø–∏—Å–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –Ω–∞–ø–∏—à–∏—Ç–µ —Ç–µ–∫—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –ø–æ —Å–ª–µ–¥—É—é—â–µ–º—É –∑–∞–¥–∞–Ω–∏—é.\\n\\n### Input:\\n–ù–∞–ø–∏—à–∏—Ç–µ —Ç–µ–∫—Å—Ç, –æ–±—ä—è—Å–Ω—è—é—â–∏–π —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É —Ç–µ—Ä–º–∏–Ω–∞–º–∏ \"–∑–∞–≥—Ä—É–∑–∫–∞\" –∏ \"—Å–∫–∞—á–∏–≤–∞–Ω–∏–µ\" –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Ä–∞–±–æ—Ç—ã —Å –¥–∞–Ω–Ω—ã–º–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.\\n\\n### Response:\\n–ó–∞–≥—Ä—É–∑–∫–∞ –∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ\\n–õ—é–±–æ–ø—ã—Ç–Ω–æ, —á—Ç–æ –≤ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –ø—Ä–æ—Ü–µ—Å—Å –ø–µ—Ä–µ–¥–∞—á–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–∞—Ä—É–∂—É (–Ω–∞ —Å–µ—Ä–≤–µ—Ä –∏–ª–∏ –≤ –ø–∞–º—è—Ç—å) –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è \"–∑–∞–≥—Ä—É–∑–∫–∞\", –∞ –ø—Ä–æ—Ü–µ—Å—Å –ø–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑–≤–Ω–µ - \"—Å–∫–∞—á–∏–≤–∞–Ω–∏–µ\".\\n–ü—Ä–µ–∫—Ä–∞—Å–Ω—ã–π –ø—Ä–∏–º–µ—Ä: \"–°–∫–∞—á–∞—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º—ã –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ó–∞–≥—Ä—É–∑–∫–∞\".\\n–¢–æ –µ—Å—Ç—å, –Ω–∞—Ä—É–∂—É –º—ã –≥—Ä—É–∑–∏–º, —Å–æ–≤–µ—Ä—à–∞–µ–º –¥–µ–π—Å—Ç–≤–∏–µ —Å —á–µ–º-—Ç–æ —Ç–≤–µ—Ä–¥—ã–º. –ê –∫ —Å–µ–±–µ –º—ã —Å–∫–∞—á–∏–≤–∞–µ–º, —Ç–æ –µ—Å—Ç—å, —Å–æ–≤–µ—Ä—à–∞–µ–º –¥–µ–π—Å—Ç–≤–∏–µ —Å —á–µ–º-—Ç–æ –∂–∏–¥–∫–∏–º –∏ –±–µ—Å—Ñ–æ—Ä–º–µ–Ω–Ω—ã–º.<|eot_id|>'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")\n",
    "\n",
    "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
    "EOS_TOKEN = tokenizer.eos_token  # –î–æ–±–∞–≤–ª—è–µ–º EOS-—Ç–æ–∫–µ–Ω (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ formatting_prompts_func)\n",
    "\n",
    "formatting_dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "formatting_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "54413241-bdd0-4f43-b3ac-a05961c6b58d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f89b4b4ae7604723a0c1bf0fa824588b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"]:   0%|          | 0/5436 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç—Ä–µ–Ω–µ—Ä–∞ –¥–ª—è Llama 3.1\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=formatting_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 1,\n",
    "    packing = False,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 10,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir=\"outputs/llama31\",\n",
    "        report_to=[\"tensorboard\"],\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "955ca78e-ea93-4419-b79e-9c0ea39c8f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4070 Ti SUPER.\n",
      "–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è 15.547 GB –∏–∑ 15.992 GB vRAM.\n"
     ]
    }
   ],
   "source": [
    "# –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–µ–∫—É—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É vRAM\n",
    "start_gpu_memory = display_vram_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84abba6c-1c51-4e2e-8262-e686dca80256",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 5,436 | Num Epochs = 1 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 41,943,040/8,000,000,000 (0.52% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 02:47, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.093800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.539000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.541900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.468500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.456000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.519700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4b7c68d3-810d-4622-92cf-8707d36b23b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–û–±—É—á–µ–Ω–∏–µ –¥–ª–∏–ª–æ—Å—å 170.8137 —Å–µ–∫—É–Ω–¥ (2.85 –º–∏–Ω—É—Ç).\n",
      "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–æ—Å—å vRAM = 15.547 –ì–ë.\n",
      "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–æ—Å—å vRAM –¥–ª—è –æ–±—É—á–µ–Ω–∏—è = 0.0 –ì–ë.\n",
      "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–æ—Å—å vRAM –≤ % –æ—Ç –æ–±—â–µ–π –ø–∞–º—è—Ç–∏ = 97.217 %.\n",
      "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–æ—Å—å vRAM –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤ % –æ—Ç –æ–±—â–µ–π –ø–∞–º—è—Ç–∏ = 0.0 %.\n"
     ]
    }
   ],
   "source": [
    "# –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–∞–º—è—Ç–∏ –∏ –≤—Ä–µ–º–µ–Ω–∏\n",
    "display_vram_stats(\"after\", start_gpu_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e012ec-934d-41f6-961b-9ffec0785d12",
   "metadata": {},
   "source": [
    "## –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å Llama 3.1 –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5e4e0033-2290-440e-8f7b-471632a83072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "–õ–∏—á–Ω–∞—è –≥–∏–≥–∏–µ–Ω–∞ –≤ –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö\n",
      "–û—á–µ–Ω—å –≤–∞–∂–Ω—ã–π –≤–æ–ø—Ä–æ—Å. –°—á–∏—Ç–∞–µ—Ç—Å—è, —á—Ç–æ –ª–∏—á–Ω–∞—è –≥–∏–≥–∏–µ–Ω–∞ - —ç—Ç–æ –≤—Å–µ, —á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ –¥—É—à–µ –∏ –Ω–∞ –±–∞–Ω–Ω—ã—Ö –∫–∞—Ç–∞—Ñ–∞–ª–∫–∞—Ö, –≤ —Å–∞–Ω—É–∑–ª–µ, –¥—É—à–µ–≤–æ–π –∏ –Ω–∞ –ø–æ–º–æ–π–∫–µ, –∞ –≤ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —á–∞—Å—Ç—è—Ö –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º–µ—Å—Ç –Ω—É–∂–Ω–æ –æ–±—Ä–∞—â–∞—Ç—å—Å—è –∫–∞–∫ –º–æ–∂–Ω–æ –º–µ–Ω—å—à–µ. \n",
      "–í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –ø—Ä–∏ –ø–æ—Å–µ—â–µ–Ω–∏–∏ –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç—É–∞–ª–µ—Ç–æ–≤ –Ω—É–∂–Ω–æ —Ç–æ–ª—å–∫–æ –æ–ø—É—Å—Ç–∏—Ç—å –Ω–∞—Ö—É–π, –Ω–∏ –ø—Ä–∏ —á–µ–º –≤ –¥–∞–ª—å–Ω–µ–π—à–µ–º –≤ —Ç—É–∞–ª–µ—Ç–µ –Ω–µ –ø–æ–¥–ª–∏–∑–∞—Ç—å—Å—è, –Ω–µ –ø–∏–∑–¥–∏—Ç—å, –Ω–µ –ø–æ—Ç—Ä–æ–≥–∞—Ç—å —Å–µ–±—è, –Ω–µ –≤–∑–¥–æ—Ö–Ω—É—Ç—å –≤ —Å—Ç–æ—Ä–æ–Ω—É —Å–∏—Å–∫–∞.\n",
      "–í–æ –≤—Å–µ–º –æ—Å—Ç–∞–ª—å–Ω–æ–º –Ω–µ –≤–∞–∂–Ω–æ. –í –±–∞—Å—Å–µ–π–Ω–∞—Ö –º–æ–∂–Ω–æ –µ—Å—Ç—å, –∫—É—Ä–∏—Ç—å, –ø–∏—Ç—å, –∂—Ä–∞—Ç—å, –¥—Ä–∞—Ç—å –∏ –ø–æ–¥–ª–∏–∑—ã–≤–∞—Ç—å—Å—è –∫–∞–∫ —É–≥–æ–¥–Ω–æ. –ù–æ —Ç–æ–ª—å–∫–æ –Ω–∞ –ø–æ–º–æ–π–∫–µ –º–æ–∂–Ω–æ –Ω–µ –ø–∏–∑–¥–∏—Ç—å. –ò —Ç–æ–ª—å–∫–æ –Ω–∞ –ø–æ–º–æ–π–∫–µ –Ω—É–∂–Ω–æ –≤–æ–æ–±—â–µ –Ω–µ –ø–æ–¥–ª–∏–∑—ã–≤–∞—Ç—å—Å—è. –ê –Ω–∞ –≤—Å–µ—Ö –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö –≤ –±–∞—Å—Å–µ–π–Ω–∞—Ö - –∫–∞–∫ —Ö–æ—á–µ—à—å.\n",
      "–ò –Ω–µ –≤–∞–∂–Ω–æ, —á–µ–º —Ç—ã –∑–∞–Ω—è–ª—Å—è –≤ —Å–∞–Ω—É–∑–ª–µ –∏–ª–∏ –¥—É—à–µ –∏–ª–∏ –ø–æ–º–æ–π–∫–µ, –≤—Å–µ –±—É–¥–µ—Ç –ø–æ–ª–µ–∑–Ω–æ –∏ –ø–æ–ª–µ–∑–Ω–µ–µ, —á–µ–º –µ—Å–ª–∏ –±—ã —Ç—ã –ø–æ–¥–ª–∏–∑—ã–≤–∞–ª—Å—è –∏–ª–∏ –¥—Ä–æ—á–∏–ª –∏–ª–∏ –ø–∏–∑–¥–∏–ª –∏–ª–∏ –≤–∑–¥–æ—Ö–Ω—É–ª –≤ —Å—Ç–æ—Ä–æ–Ω—É —Å–∏—Å–∫–∞. \n",
      "–ì–¥–µ –≤—ã –Ω–∞—Ö–æ–¥–∏—Ç–µ—Å—å –≤ –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö, –ª–∏—á–Ω–∞—è –≥–∏–≥–∏–µ–Ω–∞ –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ —Å–º–æ–∂–µ—Ç –±—ã—Ç—å —á–µ–º-—Ç–æ –¥—É—Ä–Ω—ã–º. \n",
      "–ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –ª—é–¥–µ–π –≤—Å–µ —Ä–∞–≤–Ω–æ –Ω–∏–∫–∞–∫–∏—Ö –ª–∏—á–Ω—ã—Ö –≥–∏–≥–∏–µ–Ω–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ—Ü–µ–¥—É—Ä –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ —Å–æ–±–ª—é–¥–∞—é—Ç –Ω–∞ –ª—é–¥—è—Ö. –í —Ç–∞–ø–æ—á–∫–µ –æ—Ç—Ä—ã—Ü–∏—Ç–µ –±—é–≤–µ–ª–µ –≤ —Å—Ç–æ–ª–æ–≤–æ–π. –ò –Ω–µ –ø–∏–∑–¥–∏—Ç–µ –Ω–∏ –≤ —á—ë–º.\n",
      "–í—Å–µ –±—É–¥–µ—Ç —Ö–æ—Ä–æ—à–æ!<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"–í—ã —è–≤–ª—è–µ—Ç–µ—Å—å –ø–∏—Å–∞—Ç–µ–ª–µ–º –∫–Ω–∏–≥ –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –±–ª–æ–≥–≥–µ—Ä–æ–º –∂–∏–≤–æ–≥–æ –∂—É—Ä–Ω–∞–ª–∞, —Å–æ —Å–≤–æ–∏–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–º —Å—Ç–∏–ª–µ–º –Ω–∞–ø–∏—Å–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –Ω–∞–ø–∏—à–∏—Ç–µ —Ç–µ–∫—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –ø–æ —Å–ª–µ–¥—É—é—â–µ–º—É –∑–∞–¥–∞–Ω–∏—é.\", # instruction\n",
    "        \"–ù–∞–ø–∏—Å–∞—Ç—å —Ç–µ–∫—Å—Ç, –æ–±—ä—è—Å–Ω—è—é—â–∏–π, –ø–æ—á–µ–º—É –≤–∞–∂–Ω–æ —Å–æ–±–ª—é–¥–∞—Ç—å –ø—Ä–∞–≤–∏–ª–∞ –ª–∏—á–Ω–æ–π –≥–∏–≥–∏–µ–Ω—ã –≤ –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö.\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=500,\n",
    "    temperature = 1.5, min_p = 0.1,\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "print(response[0].split(\"### Response:\")[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8427faff-f2db-4e62-bc00-8e28249ddfd0",
   "metadata": {},
   "source": [
    "## –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–¥–∞–ø—Ç–µ—Ä—ã LoRA –∏ –º–æ–¥–µ–ª—å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "27fc785a-8eea-4dd3-90e9-5a257d38158c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 74.86 out of 127.91 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:13<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Done.\n",
      "–û—á–∏—Å—Ç–∫–∞ VRAM...\n",
      "VRAM —É—Å–ø–µ—à–Ω–æ –æ—á–∏—â–µ–Ω–∞.\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"Lebedev_Llama31\")\n",
    "tokenizer.save_pretrained(\"Lebedev_Llama31\")\n",
    "model.save_pretrained_merged(\"Lebedev_Llama31-finetune\", tokenizer)\n",
    "del model\n",
    "del tokenizer\n",
    "del inputs\n",
    "del outputs\n",
    "del trainer\n",
    "del response\n",
    "del trainer_stats\n",
    "del formatting_dataset\n",
    "clear_vram()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf253f8-7b80-4de8-a94f-8d4ceaea330f",
   "metadata": {},
   "source": [
    "# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∏ –≤—ã–≤–æ–¥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c05b20-8a8d-4ace-a7db-75f2dd97ddaf",
   "metadata": {},
   "source": [
    "### –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π\n",
    "\n",
    "1. DeepSeek R1:\n",
    "\n",
    "   ‚Ä¢ –î–æ –æ–±—É—á–µ–Ω–∏—è: –ú–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –≤ –∑–∞–¥–∞–Ω–Ω–æ–º —Å—Ç–∏–ª–µ, –Ω–æ –≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–ª—É—á–∞—è—Ö –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞ —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–π –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å—Ç–∏–ª—å.\n",
    "\n",
    "   ‚Ä¢ –ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è: –ú–æ–¥–µ–ª—å —Å–º–æ–≥–ª–∞ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫ —Å—Ç–∏–ª—é –±–ª–æ–≥–≥–µ—Ä–∞, –≤–∫–ª—é—á–∞—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –ª–µ–∫—Å–∏–∫–∏. –û–¥–Ω–∞–∫–æ –≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–ª—É—á–∞—è—Ö —Ç–µ–∫—Å—Ç –±—ã–ª —á—Ä–µ–∑–º–µ—Ä–Ω–æ –≥—Ä—É–±—ã–º –∏ –≤—ã—Ö–æ–¥–∏–ª –∑–∞ —Ä–∞–º–∫–∏ –ø—Ä–∏–µ–º–ª–µ–º–æ–≥–æ —Ç–æ–Ω–∞.\n",
    "\n",
    "2. Gemma 2:\n",
    "\n",
    "   ‚Ä¢ –î–æ –æ–±—É—á–µ–Ω–∏—è: –ú–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª–∞ —Ç–µ–∫—Å—Ç—ã –≤ –±–æ–ª–µ–µ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–π –∏ —Ñ–æ—Ä–º–∞–ª—å–Ω–æ–π –º–∞–Ω–µ—Ä–µ, —á—Ç–æ –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞–ª–æ –∑–∞–¥–∞–Ω–Ω–æ–º—É —Å—Ç–∏–ª—é.\n",
    "\n",
    "   ‚Ä¢ –ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è: –ú–æ–¥–µ–ª—å —É–ª—É—á—à–∏–ª–∞ —Å—Ç–∏–ª—å, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–ª–∞ –Ω–µ–∫–æ—Ç–æ—Ä—É—é –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ—Å—Ç—å. –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è –ª–µ–∫—Å–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å —Ä–µ–∂–µ, —á–µ–º –≤ DeepSeek R1.\n",
    "\n",
    "3. Llama 3.1:\n",
    "\n",
    "   ‚Ä¢ –î–æ –æ–±—É—á–µ–Ω–∏—è: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –±—ã–ª–∞ —Ñ–æ—Ä–º–∞–ª—å–Ω–æ–π, —Å –Ω–µ–∫–æ—Ç–æ—Ä—ã–º–∏ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è–º–∏ –≤ —Å—Ç–æ—Ä–æ–Ω—É –∏–∑–±—ã—Ç–æ—á–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.\n",
    "\n",
    "   ‚Ä¢ –ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è: –ú–æ–¥–µ–ª—å –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–ª–∞—Å—å –∫ –∑–∞–¥–∞–Ω–Ω–æ–º—É —Å—Ç–∏–ª—é, –Ω–æ –∏–Ω–æ–≥–¥–∞ —Ç–µ–∫—Å—Ç —Å—Ç–∞–Ω–æ–≤–∏–ª—Å—è —Å–ª–∏—à–∫–æ–º –≥—Ä—É–±—ã–º –∏ –Ω–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º. –í –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–ª—É—á–∞—è—Ö –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª–∞ –Ω–µ–ø—Ä–∏–µ–º–ª–µ–º—ã–µ –≤—ã—Å–∫–∞–∑—ã–≤–∞–Ω–∏—è.\n",
    "\n",
    "---\n",
    "\n",
    "### –ò—Ç–æ–≥–æ–≤—ã–π –≤—ã–≤–æ–¥\n",
    "\n",
    "‚Ä¢ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: DeepSeek R1. –û–Ω–∞ –ª—É—á—à–µ –≤—Å–µ–≥–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–ª–∞—Å—å –∫ –∑–∞–¥–∞–Ω–Ω–æ–º—É —Å—Ç–∏–ª—é (–ø—Ä–æ—Å—Ç–æ–π —è–∑—ã–∫, –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è –ª–µ–∫—Å–∏–∫–∞). –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –≥—Ä—É–±—ã–µ –æ—à–∏–±–∫–∏, –º–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∞ –Ω–∞–∏–±–æ–ª–µ–µ –±–ª–∏–∑–∫–∏–π –∫ –∑–∞–¥–∞–Ω–Ω–æ–º—É —Å—Ç–∏–ª—å —Ç–µ–∫—Å—Ç–∞.\n",
    "\n",
    "‚Ä¢ Gemma 2 —Ç–∞–∫–∂–µ –ø–æ–∫–∞–∑–∞–ª–∞ —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –Ω–æ –µ–µ —Ç–µ–∫—Å—Ç—ã –±—ã–ª–∏ –±–æ–ª–µ–µ —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–º–∏, —Ö–æ—Ç—å –∏ –≤ –±–æ–ª—å—à–µ–π —Å—Ç–µ–ø–µ–Ω–∏ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–º–∏.\n",
    "\n",
    "‚Ä¢ Llama 3.1 –æ–∫–∞–∑–∞–ª–∞—Å—å —Å–∞–º–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ–π, –Ω–æ —Ç–µ–∫—Å—Ç—ã –±—ã–ª–∏ —Å–ª–∏—à–∫–æ–º –≥—Ä—É–±—ã–º–∏, –Ω–µ —Å–≤—è–∑–∞–Ω–Ω—ã–º–∏, –∏ –Ω–µ –≤—Å–µ–≥–¥–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞–ª–∏ —Å—Ç–∏–ª—é.\n",
    "\n",
    "–î–ª—è –∑–∞–¥–∞—á–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –≤ —Å—Ç–∏–ª–µ –±–ª–æ–≥–≥–µ—Ä–∞ –±–æ–ª—å—à–µ –ø–æ–¥—Ö–æ–¥–∏—Ç DeepSeek R1, —Ç–∞–∫ –∫–∞–∫ –æ–Ω–∞ –ª—É—á—à–µ –≤—Å–µ–≥–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞–ª–∞ —Å—Ç–∏–ª—é –∏ –∑–∞–¥–∞—á–µ. –û–¥–Ω–∞–∫–æ, –µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è –±–æ–ª–µ–µ –º—è–≥–∫–∏–π —Å—Ç–∏–ª—å —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è–º–∏, –º–æ–∂–Ω–æ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å Gemma 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e574d15-45c1-47be-9c8f-6c2ee0317675",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
